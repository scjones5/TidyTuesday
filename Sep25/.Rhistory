)
train_datagen <- image_data_generator(rescale = 1/255)
validation_datagen <- image_data_generator(rescale = 1/255)
train_generator <- flow_images_from_directory(
train_dir,
train_datagen,
target_size = c(150, 150),
batch_size = 20,
class_mode = "binary"
)
validation_generator <- flow_images_from_directory(
validation_dir,
validation_datagen,
target_size = c(150, 150),
batch_size = 20,
class_mode = "binary"
)
batch <- generator_next(train_generator)
str(batch)
datagen <- image_data_generator(
rescale = 1/255,
rotation_range = 40,
width_shift_range = 0.2,
height_shift_range = 0.2,
shear_range = 0.2,
zoom_range = 0.2,
horizontal_flip = TRUE,
fill_mode = "nearest"
)
fnames <- list.files(train_cats_dir, full.names = TRUE)
img_path <- fnames[[2]]
img <- image_load(img_path, target_size = c(150, 150))
img_array <- image_to_array(img)
img_array <- array_reshape(img_array, c(1, 150, 150, 3))
augmentation_generator <- flow_images_from_data(
img_array,
generator = datagen,
batch_size = 1
)
op <- par(mfrow = c(2, 2), pty = "s", mar = c(1, 0, 1, 0))
for (i in 1:4) {
batch <- generator_next(augmentation_generator)
plot(as.raster(batch[1,,,]))
}
par(op)
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu",
input_shape = c(150, 150, 3)) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_flatten() %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 512, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = optimizer_rmsprop(lr = 1e-4),
metrics = c("acc")
)
datagen <- image_data_generator(
rescale = 1/255,
rotation_range = 40,
width_shift_range = 0.2,
height_shift_range = 0.2,
shear_range = 0.2,
zoom_range = 0.2,
horizontal_flip = TRUE
)
test_datagen <- image_data_generator(rescale = 1/255)
train_generator <- flow_images_from_directory(
train_dir,
datagen,
target_size = c(150, 150),
batch_size = 32,
class_mode = "binary"
)
validation_generator <- flow_images_from_directory(
validation_dir,
test_datagen,
target_size = c(150, 150),
batch_size = 32,
class_mode = "binary"
)
history <- model %>% fit_generator(
train_generator,
steps_per_epoch = 100,
epochs = 100,
validation_data = validation_generator,
validation_steps = 50
)
model %>% save_model_hdf5("cats_and_dogs_small_2.h5")
plot(history)
conv_base <- application_vgg16(
weights = "imagenet",
include_top = FALSE,
input_shape = c(150, 150, 3)
)
summary(conv_base)
base_dir
train_dir
validation_dir
test_dir
datagen <- image_data_generator(rescale = 1/255)
batch_size <- 20
extract_features <- function(directory, sample_count) {
features <- array(0, dim = c(sample_count, 4, 4, 512))
labels <- array(0, dim = c(sample_count))
generator <- flow_images_from_directory(
directory = directory,
generator = datagen,
target_size = c(150, 150),
batch_size = batch_size,
class_mode = "binary"
)
i <- 0
while(TRUE) {
batch <- generator_next(generator)
inputs_batch <- batch[[1]]
labels_batch <- batch[[2]]
features_batch <- conv_base %>% predict(inputs_batch)
index_range <- ((i * batch_size)+1):((i + 1) * batch_size)
features[index_range,,,] <- features_batch
labels[index_range] <- labels_batch
i <- i + 1
if (i * batch_size >= sample_count)
break
}
list(
features = features,
labels = labels
)
}
train <- extract_features(train_dir, 2000)
validation <- extract_features(validation_dir, 1000)
test <- extract_features(test_dir, 1000)
reshape_features <- function(features) {
array_reshape(features, dim = c(nrow(features), 4 * 4 * 512))
}
train$features <- reshape_features(train$features)
validation$features <- reshape_features(validation$features)
test$features <- reshape_features(test$features)
model <- keras_model_sequential() %>%
layer_dense(units = 256, activation = "relu",
input_shape = 4 * 4 * 512) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
optimizer = optimizer_rmsprop(lr = 2e-5),
loss = "binary_crossentropy",
metrics = c("accuracy")
)
history <- model %>% fit(
train$features, train$labels,
epochs = 30,
batch_size = 20,
validation_data = list(validation$features, validation$labels)
)
plot(history)
model <- load_model_hdf5("~/Downloads/deep-learning-with-r-notebooks-master/cats_and_dogs_small_2.h5")
model
img_path <- "~/Downloads/cats_and_dogs_small/test/cats/cat.1700.jpg"
img <- image_load(img_path, target_size = c(150, 150))
img_tensor <- image_to_array(img)
img_tensor <- array_reshape(img_tensor, c(1, 150, 150, 3))
img_tensor <- img_tensor / 255
dim(img_tensor)
plot(as.raster(img_tensor[1,,,]))
layer_outputs <- lapply(model$layers[1:8], function(layer) layer$output)
activation_model <- keras_model(inputs = model$input, outputs = layer_outputs)
activations <- activation_model %>% predict(img_tensor)
first_layer_activation <- activations[[1]]
dim(first_layer_activation)
plot_channel <- function(channel) {
rotate <- function(x) t(apply(x, 2, rev))
image(rotate(channel), axes = FALSE, asp = 1,
col = terrain.colors(12))
}
plot_channel(first_layer_activation[1,,,2])
plot_channel(first_layer_activation[1,,,7])
image_size <- 58
images_per_row <- 16
for (i in 1:8) {
layer_activation <- activations[[i]]
layer_name <- model$layers[[i]]$name
n_features <- dim(layer_activation)[[4]]
n_cols <- n_features %/% images_per_row
png(paste0("cat_activations_", i, "_", layer_name, ".png"),
width = image_size * images_per_row,
height = image_size * n_cols)
op <- par(mfrow = c(n_cols, images_per_row), mai = rep_len(0.02, 4))
for (col in 0:(n_cols-1)) {
for (row in 0:(images_per_row-1)) {
channel_image <- layer_activation[1,,,(col*images_per_row) + row + 1]
plot_channel(channel_image)
}
}
par(op)
dev.off()
}
model <- application_vgg16(
weights = "imagenet",
include_top = FALSE
)
layer_name <- "block3_conv1"
filter_index <- 1
layer_output <- get_layer(model, layer_name)$output
loss <- k_mean(layer_output[,,,filter_index])
grads <- k_gradients(loss, model$input)[[1]]
grads <- grads / (k_sqrt(k_mean(k_square(grads))) + 1e-5)
iterate <- k_function(list(model$input), list(loss, grads))
c(loss_value, grads_value) %<-% iterate(list(array(0, dim = c(1, 150, 150, 3))))
input_img_data <- array(runif(150 * 150 * 3), dim = c(1, 150, 150, 3)) * 20 + 128
step <- 1
for (i in 1:40) {
c(loss_value, grads_value) %<-% iterate(list(input_img_data))
input_img_data <- input_img_data + (grads_value * step)
}
# Clear out the session
k_clear_session()
# Note that we are including the densely-connected classifier on top;
# all previous times, we were discarding it.
model <- application_vgg16(weights = "imagenet")
img_path <- "~/Downloads/creative_commons_elephant.jpg"
# Start witih image of size 224 Ã— 224
img <- image_load(img_path, target_size = c(224, 224)) %>%
# Array of shape (224, 224, 3)
image_to_array() %>%
# Adds a dimension to transform the array into a batch of size (1, 224, 224, 3)
array_reshape(dim = c(1, 224, 224, 3)) %>%
# Preprocesses the batch (this does channel-wise color normalization)
imagenet_preprocess_input()
imdb_dir <- "~/Downloads/ac1Imdb"
train_dir <- file.path(imdb_dir, "train")
labels <- c()
texts <- c()
for (label_type in c("neg", "pos")) {
label <- switch(label_type, neg = 0, pos = 1)
dir_name <- file.path(train_dir, label_type)
for (fname in list.files(dir_name, pattern = glob2rx("*.txt"),
full.names = TRUE)) {
texts <- c(texts, readChar(fname, file.info(fname)$size))
labels <- c(labels, label)
}
}
labels
imdb_dir <- "~/Downloads/aclImdb"
for (label_type in c("neg", "pos")) {
label <- switch(label_type, neg = 0, pos = 1)
dir_name <- file.path(train_dir, label_type)
for (fname in list.files(dir_name, pattern = glob2rx("*.txt"),
full.names = TRUE)) {
texts <- c(texts, readChar(fname, file.info(fname)$size))
labels <- c(labels, label)
}
}
texts
train_dir <- file.path(imdb_dir, "train")
for (label_type in c("neg", "pos")) {
label <- switch(label_type, neg = 0, pos = 1)
dir_name <- file.path(train_dir, label_type)
for (fname in list.files(dir_name, pattern = glob2rx("*.txt"),
full.names = TRUE)) {
texts <- c(texts, readChar(fname, file.info(fname)$size))
labels <- c(labels, label)
}
}
texts
maxlen <- 100
training_samples <- 200
validation_samples <- 10000
max_words <- 10000
tokenizer <- text_tokenizer(num_words = max_words) %>%
fit_text_tokenizer(texts)
sequences <- texts_to_sequences(tokenizer, texts)
word_index = tokenizer$word_index
cat("Found", length(word_index), "unique tokens.\n")
data <- pad_sequences(sequences, maxlen = maxlen)
labels <- as.array(labels)
cat("Shape of data tensor:", dim(data), "\n")
cat("Shape of label tensor:", dim(labels), "\n")
indices <- sample(1:nrow(data))
training_indices <- indices[1:training_samples]
?indices
??indices
validation_indices <- indices[(training_samples + 1): (training_samples + validation_samples)]
x_train <- data[training_indices,]
y_train <- labels[training_indices]
x_val <- data[validation_indices,]
y_val <- labels[validation_indices]
glove_dir <- "~/Downloads/glove.6B"
lines <- readLines(file.path(glove_dir, "glove.6B.100d.txt"))
embeddings_index <- new.env(hash = TRUE, parent = emptyenv())
for (i in 1:length(lines)) {
line <- lines[[i]]
values <- strsplit(line, " ")[[1]]
word <- values[[1]]
embeddings_index[[word]] <- as.double(values[-1])
}
embedding_dim <- 100
embedding_matrix <- array(0, c(max_words, embedding_dim))
for (word in names(word_index)) {
index <- word_index[[word]]
if (index < max_words) {
embedding_vector <- embeddings_index[[word]]
if (!is.null(embedding_vector))
embedding_matrix[index+1,] <- embedding_vector
}
}
model <- keras_model_sequential() %>%
layer_embedding(input_dim = max_words, output_dim = embedding_dim,
input_length = maxlen) %>%
layer_flatten() %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
summary(model)
get_layer(model, index = 1) %>%
set_weights(list(embedding_matrix)) %>%
freeze_weights()
str(embedding_matrix)
get_layer(model, index = 0) %>%
set_weights(list(embedding_matrix)) %>%
freeze_weights()
model %>% compile(
optimizer = "rmsprop",
loss = "binary_crossentropy",
metrics = c("acc")
)
history <- model %>% fit(
x_train, y_train,
epochs = 20,
batch_size = 32,
validation_data = list(x_val, y_val)
)
save_model_weights_hdf5(model, "~/Downloads/pre_trained_glove_model.h5")
plot(history)
test_dir <- file.path(imdb_dir, "test")
labels <- c()
texts <- c()
for (label_type in c("neg", "pos")) {
label <- switch(label_type, neg=0, pos=1)
dir_name <- file.path(test_dir, label_type)
for (fname in list.files(dir_name, pattern = glob2rx("*.txt"),
full.names = TRUE)) {
texts <- c(texts, readChar(fname, file.info(fname)$size))
labels <- c(labels, label)
}
}
sequences <- texts_to_sequences(tokenizer, texts)
x_test <- pad_sequences(sequences, maxlen = maxlen)
y_test <- as.array(labels)
model %>% load_model_weights_hdf5("~/Downloads/pre_trained_glove_model.h5") %>%
evaluate(x_test, y_test)
library(devtools)
devtools::install_github("AndreaCirilloAC/updateR")
library(updateR)
library(dplyr)
updateR(admin_password = "roger1&WWW")
remove.packages("dplyr")
devtools::install_github("AndreaCirilloAC/updateR")
library(updateR)
library(dplyr)
library(dplyr)
??tmap
data(world)
install.packages("countrycode")
library(countrycode)
countrycode(sourcevar = "Canada", origin = "country.name", destination = "continent")
setwd("/Users/sjones/Google Drive/tidyTuesday/Sep25")
threat <- read_csv("./table_2.csv")
source <- read_csv("./table_4.csv")
??read_csv
library(readr)
threat <- read_csv("./table_2.csv")
source <- read_csv("./table_4.csv")
str(source)
threat <- threat %>% mutate(tcostbil = invasion_cost/1000000000)
source <- source %>% mutate(scostbil = invasion_cost/1000000000)
threat <- factor(countrycode(sourcevar = threat$country,
origin = "country.name",
destination = "continent"))
str(threat)
threat <- read_csv("./table_2.csv")
source <- read_csv("./table_4.csv")
threat <- threat %>% mutate(tcostbil = invasion_cost/1000000000)
source <- source %>% mutate(scostbil = invasion_cost/1000000000)
threat$continent <- factor(countrycode(sourcevar = threat$country,
origin = "country.name",
destination = "continent"))
str(threat)
threat$continent[threat$country == "Czech Republic"]
threat$continent[threat$country == "Czech"]
threat$continent[threat$country == "Republic"]
threat <- threat %>% mutate(tcostbil = invasion_cost/1000000000)
source <- source %>% mutate(scostbil = invasion_cost/1000000000)
threat$continent <- factor(countrycode(sourcevar = threat[,country],
origin = "country.name",
destination = "continent"))
threat <- threat %>% mutate(tcostbil = invasion_cost/1000000000)
source <- source %>% mutate(scostbil = invasion_cost/1000000000)
threat$continent <- factor(countrycode(sourcevar = threat[,"country"],
origin = "country.name",
destination = "continent"))
names(threat)
str(threat[,"country"])
threat$continent <- factor(countrycode(sourcevar = threat[,"country"],
origin = "country.name",
destination = "continent"))
countrycode(sourcevar = "Czech Republic", origin = "country.name", destination = "continent")
threat$continent <- factor(countrycode(sourcevar = threat$country,
origin = "country.name",
destination = "continent"))
threat$continent[threat$country == "Czech"]
threat[threat$country == "Czech",]
threat$continent[threat$country == "Czech"] <- "Europe"
cost_df <- merge(threat, source, by = "country")
str(cost_df)
str(threat)
length(source$country)
length(threat$country)
?merge
cost_df <- merge(threat, source, by = "country")
cost_df <- cost_df %>% filter(continent == "Europe")
library(tidyverse)
eur_df <- cost_df %>% filter(continent == "Europe")
ggplot(eur_df, aes(tcostbil, scostbil)) + geom_point()
str(eur_df)
threat <- read_csv("./table_2.csv")
source <- read_csv("./table_4.csv")
threat <- threat %>% mutate(tcostbil = invasion_cost/1000000000)
source <- source %>% mutate(scostbil = invasion_cost/1000000000)
threat$continent <- factor(countrycode(sourcevar = threat$country,
origin = "country.name",
destination = "continent"))
#Need to put in Czech Republic by hand
threat$continent[threat$country == "Czech"] <- "Europe"
cost_df <- merge(threat, source, by = "country")
eur_df <- cost_df %>% filter(continent == "Europe")
str(eur_df)
eur_df <- cost_df %>% filter(continent == "Europe") %>% select(country, tcostbil, scostbil)
ggplot(eur_df, aes(tcostbil, scostbil)) + geom_point()
t3 <- read_csv("./table_3.csv")
str(t3)
threat <- read_csv("./table_2.csv")
source <- read_csv("./table_4.csv")
t3 <- read_csv("./table_3.csv")
threat <- threat %>% mutate(tcostbil = invasion_cost/1000000000)
source <- source %>% mutate(scostbil = invasion_cost/1000000000)
threat$continent <- factor(countrycode(sourcevar = threat$country,
origin = "country.name",
destination = "continent"))
#Need to put in Czech Republic by hand
threat$continent[threat$country == "Czech"] <- "Europe"
cost_df <- merge(threat, source, by = "country")
cost_df <- merge(cost_df, t3[, "gdp_mean"], by = "country")
eur_df <- cost_df %>% filter(continent == "Europe") %>% select(country, tcostbil, scostbil)
str(t3)
threat <- read_csv("./table_2.csv")
source <- read_csv("./table_4.csv")
t3 <- read_csv("./table_3.csv")
threat <- threat %>% mutate(tcostbil = invasion_cost/1000000000)
source <- source %>% mutate(scostbil = invasion_cost/1000000000)
threat$continent <- factor(countrycode(sourcevar = threat$country,
origin = "country.name",
destination = "continent"))
#Need to put in Czech Republic by hand
threat$continent[threat$country == "Czech"] <- "Europe"
cost_df <- merge(threat, source, by = "country")
cost_df <- merge(cost_df, t3[, c("country", "gdp_mean")], by = "country")
eur_df <- cost_df %>% filter(continent == "Europe") %>% select(country, tcostbil, scostbil)
str(eur_df)
eur_df <- cost_df %>% filter(continent == "Europe") %>% select(country, tcostbil, scostbil, gdp_mean)
str(eur_df)
eur_df <- eur_df %>% mutate(tprop = tcostbil/gdp_mean, sprop = scostbil/gdp_mean)
str(eur_df)
ggplot(eur_df, aes(tprop, sprop)) + geom_point()
eur_df <- eur_df %>% mutate(diffbil = tcostbil - scostbil)
str(eur_df)
sum(eur_df$diffbil > 0)
sum(eur_df$diffbil < 0)
eur_df <- eur_df %>% mutate(diffbil = scostbil - tcostbil)
sum(eur_df$diffbil > 0)
?topleth
??topleth
library(maps)
?map_data
map_data("world", region = "Europe")
install.packages("cartography")
world <- rnaturalearth::countries110
install.packages("rnaturalearth")
world <- rnaturalearth::countries110
install.packages("rnaturalearth")
library(rnaturalearth)
install.packages("units")
library(rnaturalearth)
world <- rnaturalearth::countries110
europe <- world[world$region_un=="Europe"&world$name!="Russia",]
plot(europe)
europe.bbox <- SpatialPolygons(list(Polygons(list(Polygon(
matrix(c(-25,29,45,29,45,75,-25,75,-25,29),byrow = T,ncol = 2)
)), ID = 1)), proj4string = CRS(proj4string(europe)))
install.packages("SpatialPolygons")
??SpatialPolygons
library(maps)
europe.bbox <- SpatialPolygons(list(Polygons(list(Polygon(
matrix(c(-25,29,45,29,45,75,-25,75,-25,29),byrow = T,ncol = 2)
)), ID = 1)), proj4string = CRS(proj4string(europe)))
install.packages("sp")
